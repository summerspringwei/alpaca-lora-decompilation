LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=12288, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 12288])
hook
PytorchGELUTanh()
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 12288])
hook
Linear(in_features=12288, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2MLP(
  (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
  (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
  (act): PytorchGELUTanh()
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2DecoderLayer(
  (self_attn): Starcoder2SdpaAttention(
    (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (k_proj): Linear(in_features=3072, out_features=256, bias=True)
    (v_proj): Linear(in_features=3072, out_features=256, bias=True)
    (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (rotary_emb): Starcoder2RotaryEmbedding()
  )
  (mlp): Starcoder2MLP(
    (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
    (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
    (act): PytorchGELUTanh()
  )
  (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Starcoder2RotaryEmbedding()
input:
torch.Size([4, 2, 3840, 128])
output:
torch.Size([3840, 128])
torch.Size([3840, 128])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2SdpaAttention(
  (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (k_proj): Linear(in_features=3072, out_features=256, bias=True)
  (v_proj): Linear(in_features=3072, out_features=256, bias=True)
  (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (rotary_emb): Starcoder2RotaryEmbedding()
)
input:
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=12288, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 12288])
hook
PytorchGELUTanh()
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 12288])
hook
Linear(in_features=12288, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2MLP(
  (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
  (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
  (act): PytorchGELUTanh()
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2DecoderLayer(
  (self_attn): Starcoder2SdpaAttention(
    (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (k_proj): Linear(in_features=3072, out_features=256, bias=True)
    (v_proj): Linear(in_features=3072, out_features=256, bias=True)
    (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (rotary_emb): Starcoder2RotaryEmbedding()
  )
  (mlp): Starcoder2MLP(
    (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
    (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
    (act): PytorchGELUTanh()
  )
  (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Starcoder2RotaryEmbedding()
input:
torch.Size([4, 2, 3840, 128])
output:
torch.Size([3840, 128])
torch.Size([3840, 128])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2SdpaAttention(
  (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (k_proj): Linear(in_features=3072, out_features=256, bias=True)
  (v_proj): Linear(in_features=3072, out_features=256, bias=True)
  (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (rotary_emb): Starcoder2RotaryEmbedding()
)
input:
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=12288, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 12288])
hook
PytorchGELUTanh()
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 12288])
hook
Linear(in_features=12288, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2MLP(
  (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
  (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
  (act): PytorchGELUTanh()
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2DecoderLayer(
  (self_attn): Starcoder2SdpaAttention(
    (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (k_proj): Linear(in_features=3072, out_features=256, bias=True)
    (v_proj): Linear(in_features=3072, out_features=256, bias=True)
    (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (rotary_emb): Starcoder2RotaryEmbedding()
  )
  (mlp): Starcoder2MLP(
    (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
    (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
    (act): PytorchGELUTanh()
  )
  (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Starcoder2RotaryEmbedding()
input:
torch.Size([4, 2, 3840, 128])
output:
torch.Size([3840, 128])
torch.Size([3840, 128])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2SdpaAttention(
  (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (k_proj): Linear(in_features=3072, out_features=256, bias=True)
  (v_proj): Linear(in_features=3072, out_features=256, bias=True)
  (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (rotary_emb): Starcoder2RotaryEmbedding()
)
input:
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=12288, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 12288])
hook
PytorchGELUTanh()
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 12288])
hook
Linear(in_features=12288, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2MLP(
  (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
  (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
  (act): PytorchGELUTanh()
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2DecoderLayer(
  (self_attn): Starcoder2SdpaAttention(
    (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (k_proj): Linear(in_features=3072, out_features=256, bias=True)
    (v_proj): Linear(in_features=3072, out_features=256, bias=True)
    (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (rotary_emb): Starcoder2RotaryEmbedding()
  )
  (mlp): Starcoder2MLP(
    (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
    (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
    (act): PytorchGELUTanh()
  )
  (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Starcoder2RotaryEmbedding()
input:
torch.Size([4, 2, 3840, 128])
output:
torch.Size([3840, 128])
torch.Size([3840, 128])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2SdpaAttention(
  (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (k_proj): Linear(in_features=3072, out_features=256, bias=True)
  (v_proj): Linear(in_features=3072, out_features=256, bias=True)
  (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (rotary_emb): Starcoder2RotaryEmbedding()
)
input:
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=12288, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 12288])
hook
PytorchGELUTanh()
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 12288])
hook
Linear(in_features=12288, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2MLP(
  (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
  (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
  (act): PytorchGELUTanh()
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2DecoderLayer(
  (self_attn): Starcoder2SdpaAttention(
    (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (k_proj): Linear(in_features=3072, out_features=256, bias=True)
    (v_proj): Linear(in_features=3072, out_features=256, bias=True)
    (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (rotary_emb): Starcoder2RotaryEmbedding()
  )
  (mlp): Starcoder2MLP(
    (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
    (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
    (act): PytorchGELUTanh()
  )
  (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Starcoder2RotaryEmbedding()
input:
torch.Size([4, 2, 3840, 128])
output:
torch.Size([3840, 128])
torch.Size([3840, 128])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2SdpaAttention(
  (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (k_proj): Linear(in_features=3072, out_features=256, bias=True)
  (v_proj): Linear(in_features=3072, out_features=256, bias=True)
  (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (rotary_emb): Starcoder2RotaryEmbedding()
)
input:
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=12288, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 12288])
hook
PytorchGELUTanh()
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 12288])
hook
Linear(in_features=12288, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2MLP(
  (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
  (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
  (act): PytorchGELUTanh()
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2DecoderLayer(
  (self_attn): Starcoder2SdpaAttention(
    (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (k_proj): Linear(in_features=3072, out_features=256, bias=True)
    (v_proj): Linear(in_features=3072, out_features=256, bias=True)
    (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (rotary_emb): Starcoder2RotaryEmbedding()
  )
  (mlp): Starcoder2MLP(
    (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
    (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
    (act): PytorchGELUTanh()
  )
  (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Starcoder2RotaryEmbedding()
input:
torch.Size([4, 2, 3840, 128])
output:
torch.Size([3840, 128])
torch.Size([3840, 128])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2SdpaAttention(
  (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (k_proj): Linear(in_features=3072, out_features=256, bias=True)
  (v_proj): Linear(in_features=3072, out_features=256, bias=True)
  (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (rotary_emb): Starcoder2RotaryEmbedding()
)
input:
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=12288, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 12288])
hook
PytorchGELUTanh()
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 12288])
hook
Linear(in_features=12288, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2MLP(
  (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
  (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
  (act): PytorchGELUTanh()
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2DecoderLayer(
  (self_attn): Starcoder2SdpaAttention(
    (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (k_proj): Linear(in_features=3072, out_features=256, bias=True)
    (v_proj): Linear(in_features=3072, out_features=256, bias=True)
    (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (rotary_emb): Starcoder2RotaryEmbedding()
  )
  (mlp): Starcoder2MLP(
    (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
    (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
    (act): PytorchGELUTanh()
  )
  (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Starcoder2RotaryEmbedding()
input:
torch.Size([4, 2, 3840, 128])
output:
torch.Size([3840, 128])
torch.Size([3840, 128])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2SdpaAttention(
  (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (k_proj): Linear(in_features=3072, out_features=256, bias=True)
  (v_proj): Linear(in_features=3072, out_features=256, bias=True)
  (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (rotary_emb): Starcoder2RotaryEmbedding()
)
input:
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=12288, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 12288])
hook
PytorchGELUTanh()
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 12288])
hook
Linear(in_features=12288, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2MLP(
  (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
  (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
  (act): PytorchGELUTanh()
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2DecoderLayer(
  (self_attn): Starcoder2SdpaAttention(
    (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (k_proj): Linear(in_features=3072, out_features=256, bias=True)
    (v_proj): Linear(in_features=3072, out_features=256, bias=True)
    (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (rotary_emb): Starcoder2RotaryEmbedding()
  )
  (mlp): Starcoder2MLP(
    (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
    (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
    (act): PytorchGELUTanh()
  )
  (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Starcoder2RotaryEmbedding()
input:
torch.Size([4, 2, 3840, 128])
output:
torch.Size([3840, 128])
torch.Size([3840, 128])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2SdpaAttention(
  (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (k_proj): Linear(in_features=3072, out_features=256, bias=True)
  (v_proj): Linear(in_features=3072, out_features=256, bias=True)
  (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (rotary_emb): Starcoder2RotaryEmbedding()
)
input:
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=12288, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 12288])
hook
PytorchGELUTanh()
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 12288])
hook
Linear(in_features=12288, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2MLP(
  (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
  (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
  (act): PytorchGELUTanh()
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2DecoderLayer(
  (self_attn): Starcoder2SdpaAttention(
    (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (k_proj): Linear(in_features=3072, out_features=256, bias=True)
    (v_proj): Linear(in_features=3072, out_features=256, bias=True)
    (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (rotary_emb): Starcoder2RotaryEmbedding()
  )
  (mlp): Starcoder2MLP(
    (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
    (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
    (act): PytorchGELUTanh()
  )
  (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Linear(in_features=3072, out_features=256, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 256])
hook
Starcoder2RotaryEmbedding()
input:
torch.Size([4, 2, 3840, 128])
output:
torch.Size([3840, 128])
torch.Size([3840, 128])
hook
Linear(in_features=3072, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2SdpaAttention(
  (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (k_proj): Linear(in_features=3072, out_features=256, bias=True)
  (v_proj): Linear(in_features=3072, out_features=256, bias=True)
  (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
  (rotary_emb): Starcoder2RotaryEmbedding()
)
input:
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Linear(in_features=3072, out_features=12288, bias=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 12288])
hook
PytorchGELUTanh()
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 12288])
hook
Linear(in_features=12288, out_features=3072, bias=True)
input:
torch.Size([4, 3840, 12288])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2MLP(
  (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
  (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
  (act): PytorchGELUTanh()
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2DecoderLayer(
  (self_attn): Starcoder2SdpaAttention(
    (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (k_proj): Linear(in_features=3072, out_features=256, bias=True)
    (v_proj): Linear(in_features=3072, out_features=256, bias=True)
    (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
    (rotary_emb): Starcoder2RotaryEmbedding()
  )
  (mlp): Starcoder2MLP(
    (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
    (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
    (act): PytorchGELUTanh()
  )
  (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 3072])
hook
Starcoder2Model(
  (embed_tokens): Embedding(49152, 3072)
  (layers): ModuleList(
    (0-29): 30 x Starcoder2DecoderLayer(
      (self_attn): Starcoder2SdpaAttention(
        (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
        (k_proj): Linear(in_features=3072, out_features=256, bias=True)
        (v_proj): Linear(in_features=3072, out_features=256, bias=True)
        (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
        (rotary_emb): Starcoder2RotaryEmbedding()
      )
      (mlp): Starcoder2MLP(
        (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
        (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
        (act): PytorchGELUTanh()
      )
      (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
      (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
)
input:
output:
hook
Linear(in_features=3072, out_features=49152, bias=False)
input:
torch.Size([4, 3840, 3072])
output:
torch.Size([4, 3840, 49152])
hook
CrossEntropyLoss()
input:
torch.Size([15356, 49152])
torch.Size([15356])
output:
torch.Size([])
hook
Starcoder2ForCausalLM(
  (model): Starcoder2Model(
    (embed_tokens): Embedding(49152, 3072)
    (layers): ModuleList(
      (0-29): 30 x Starcoder2DecoderLayer(
        (self_attn): Starcoder2SdpaAttention(
          (q_proj): Linear(in_features=3072, out_features=3072, bias=True)
          (k_proj): Linear(in_features=3072, out_features=256, bias=True)
          (v_proj): Linear(in_features=3072, out_features=256, bias=True)
          (o_proj): Linear(in_features=3072, out_features=3072, bias=True)
          (rotary_emb): Starcoder2RotaryEmbedding()
        )
        (mlp): Starcoder2MLP(
          (c_fc): Linear(in_features=3072, out_features=12288, bias=True)
          (c_proj): Linear(in_features=12288, out_features=3072, bias=True)
          (act): PytorchGELUTanh()
        )
        (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=3072, out_features=49152, bias=False)
)